# ğŸ“Š Kibana Logging Setup Guide

## ğŸ¯ Tá»•ng quan

HÆ°á»›ng dáº«n nÃ y sáº½ giÃºp báº¡n triá»ƒn khai ELK Stack (Elasticsearch, Logstash, Kibana) Ä‘á»ƒ monitor vÃ  phÃ¢n tÃ­ch logs tá»« 4 API services sá»­ dá»¥ng Docker Compose.

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
ğŸ“¦logging
 â”£ ğŸ“‚elasticsearch
 â”ƒ â”— ğŸ“œelasticsearch.yml
 â”£ ğŸ“‚kibana
 â”ƒ â”£ ğŸ“œkibana.yml
 â”ƒ â”— ğŸ“œapi-dashboard.json
 â”£ ğŸ“‚logstash
 â”ƒ â”£ ğŸ“‚config
 â”ƒ â”ƒ â”— ğŸ“œlogstash.yml
 â”ƒ â”— ğŸ“‚pipeline
 â”ƒ â”ƒ â”£ ğŸ“œapi-logs.conf
 â”ƒ â”ƒ â”— ğŸ“œlogstash.conf
 â”£ ğŸ“œdjango_logging.py
 â”— ğŸ“œlogstash_middleware.py
```

## ğŸš€ Triá»ƒn khai

### 1. Khá»Ÿi Ä‘á»™ng ELK Stack

```bash
# Khá»Ÿi Ä‘á»™ng táº¥t cáº£ services (APIs + MongoDB + Prometheus + Grafana + ELK)
docker-compose up -d

# Kiá»ƒm tra tráº¡ng thÃ¡i cÃ¡c containers
docker-compose ps
```

### 2. Kiá»ƒm tra cÃ¡c services

#### API Services
- **User API**: http://localhost:8000
- **Product API**: http://localhost:8001  
- **Order API**: http://localhost:8002
- **Payment API**: http://localhost:8003

#### Monitoring Services
- **Prometheus**: http://localhost:9090
- **Grafana**: http://localhost:3000

#### Logging Services
- **Elasticsearch**: http://localhost:9200
- **Kibana**: http://localhost:5601

#### Database
- **MongoDB**: localhost:27017

### 3. Kiá»ƒm tra Elasticsearch

```bash
# Kiá»ƒm tra Elasticsearch health
curl http://localhost:9200/_cluster/health

# Kiá»ƒm tra indices
curl http://localhost:9200/_cat/indices
```

### 4. Kiá»ƒm tra Logstash

```bash
# Kiá»ƒm tra Logstash logs
docker-compose logs logstash

# Test gá»­i log tá»›i Logstash
echo '{"message": "test log", "service": "test-api"}' | nc localhost 5000
```

## ğŸ“Š Cáº¥u hÃ¬nh Kibana

### 1. Truy cáº­p Kibana

- **URL**: http://localhost:5601
- **KhÃ´ng cáº§n Ä‘Äƒng nháº­p** (security disabled)

### 2. Táº¡o Index Pattern

1. VÃ o **Stack Management** > **Index Patterns**
2. Click **Create index pattern**
3. Nháº­p pattern: `api-logs-*`
4. Chá»n time field: `@timestamp`
5. Click **Create index pattern**

### 3. Import Dashboard

1. VÃ o **Stack Management** > **Saved Objects**
2. Click **Import**
3. Upload file `logging/kibana/api-dashboard.json`
4. Click **Import**

### 4. Xem Dashboard

1. VÃ o **Dashboard**
2. Chá»n **API Logs Dashboard**
3. Xem cÃ¡c metrics:
   - **API Requests Over Time**: Biá»ƒu Ä‘á»“ requests theo thá»i gian
   - **Response Status Codes**: PhÃ¢n bá»‘ status codes
   - **Top Services by Request Count**: Top services cÃ³ nhiá»u requests
   - **Average Response Time by Service**: Thá»i gian pháº£n há»“i trung bÃ¬nh
   - **Recent API Logs**: Báº£ng logs gáº§n Ä‘Ã¢y

## ğŸ”§ Cáº¥u hÃ¬nh chi tiáº¿t

### Elasticsearch Configuration

File `logging/elasticsearch/elasticsearch.yml`:

```yaml
cluster.name: "api-cluster"
node.name: "api-node-1"
network.host: 0.0.0.0
discovery.type: single-node
xpack.security.enabled: false
xpack.security.enrollment.enabled: false
xpack.security.http.ssl.enabled: false
xpack.security.transport.ssl.enabled: false
xpack.monitoring.collection.enabled: true
```

### Kibana Configuration

File `logging/kibana/kibana.yml`:

```yaml
server.name: kibana
server.host: "0.0.0.0"
elasticsearch.hosts: [ "http://elasticsearch:9200" ]
monitoring.ui.container.elasticsearch.enabled: true
xpack.security.enabled: false
xpack.encryptedSavedObjects.encryptionKey: "something_at_least_32_characters_long"
```

### Logstash Configuration

File `logging/logstash/pipeline/api-logs.conf`:

```ruby
input {
  beats {
    port => 5044
  }
  
  tcp {
    port => 5000
    codec => json_lines
  }
  
  udp {
    port => 5000
    codec => json_lines
  }
}

filter {
  if [fields][service] {
    mutate {
      add_field => { "service_name" => "%{[fields][service]}" }
    }
  }
  
  # Parse Django logs
  if [message] =~ /\[.*\]/ {
    grok {
      match => { "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\] \[%{NUMBER:pid}\] \[%{LOGLEVEL:level}\] %{GREEDYDATA:log_message}" }
    }
  }
  
  # Parse HTTP request logs
  if [message] =~ /HTTP/ {
    grok {
      match => { "message" => "%{IPORHOST:client_ip} - - \[%{HTTPDATE:timestamp}\] \"%{WORD:method} %{URIPATH:request_path}(?:%{URIPARAM:request_params})? %{DATA:http_version}\" %{NUMBER:response_code} %{NUMBER:response_size}" }
    }
  }
  
  # Add timestamp
  date {
    match => [ "timestamp", "ISO8601" ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "api-logs-%{+YYYY.MM.dd}"
  }
  
  stdout {
    codec => rubydebug
  }
}
```

## ğŸ”Œ TÃ­ch há»£p vá»›i Django APIs

### 1. Cáº­p nháº­t Django Settings

ThÃªm vÃ o `settings.py` cá»§a má»—i API:

```python
# Import logging configuration
from logging_django_logstash import LOGGING

# Add to settings
LOGGING = LOGGING

# Add middleware
MIDDLEWARE = [
    'logging_django_logstash.LogstashMiddleware',
    # ... other middleware
]
```

### 2. CÃ i Ä‘áº·t dependencies

ThÃªm vÃ o `requirements.txt`:

```
elasticsearch==8.11.0
```

### 3. Táº¡o logs directory

```bash
# Trong má»—i API container
mkdir -p /app/logs
```

## ğŸ§ª Test Logging System

### 1. Táº¡o traffic test

```bash
# Táº¡o requests Ä‘á»ƒ test logging
for i in {1..50}; do
  curl http://localhost:8000/api/users/ &
  curl http://localhost:8001/api/products/ &
  curl http://localhost:8002/api/orders/ &
  curl http://localhost:8003/api/payments/ &
  sleep 1
done
```

### 2. Kiá»ƒm tra logs trong Elasticsearch

```bash
# Kiá»ƒm tra indices
curl http://localhost:9200/_cat/indices

# Xem logs má»›i nháº¥t
curl -X GET "localhost:9200/api-logs-*/_search?pretty" -H 'Content-Type: application/json' -d'
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "@timestamp": {
        "order": "desc"
      }
    }
  ],
  "size": 10
}'
```

### 3. Xem logs trong Kibana

1. Truy cáº­p http://localhost:5601
2. VÃ o **Discover**
3. Chá»n index pattern `api-logs-*`
4. Xem logs real-time

## ğŸ“Š Dashboard Features

### Real-time Log Analysis
- **Time range**: Tá»± Ä‘á»™ng cáº­p nháº­t
- **Refresh rate**: Real-time
- **Filters**: Theo service, status code, method

### Custom Visualizations
- **Line Chart**: Requests over time
- **Pie Chart**: Status codes distribution
- **Bar Chart**: Top services
- **Table**: Recent logs with details

### Log Fields Available
- `@timestamp`: Thá»i gian log
- `service`: TÃªn service (user-api, product-api, etc.)
- `level`: Log level (INFO, ERROR, WARNING)
- `message`: Ná»™i dung log
- `http.method`: HTTP method (GET, POST, etc.)
- `http.status_code`: Status code
- `http.path`: Request path
- `performance.duration_ms`: Response time
- `client_ip`: Client IP address

## ğŸ” Advanced Queries

### 1. TÃ¬m logs lá»—i

```json
{
  "query": {
    "bool": {
      "must": [
        {
          "range": {
            "http.status_code": {
              "gte": 400
            }
          }
        }
      ]
    }
  }
}
```

### 2. TÃ¬m logs theo service

```json
{
  "query": {
    "term": {
      "service": "user-api"
    }
  }
}
```

### 3. TÃ¬m logs cháº­m

```json
{
  "query": {
    "range": {
      "performance.duration_ms": {
        "gte": 1000
      }
    }
  }
}
```

## ğŸ› ï¸ Troubleshooting

### 1. Kiá»ƒm tra logs

```bash
# Xem logs cá»§a táº¥t cáº£ services
docker-compose logs

# Xem logs cá»§a service cá»¥ thá»ƒ
docker-compose logs elasticsearch
docker-compose logs kibana
docker-compose logs logstash
```

### 2. Kiá»ƒm tra káº¿t ná»‘i

```bash
# Kiá»ƒm tra Elasticsearch
curl http://localhost:9200/_cluster/health

# Kiá»ƒm tra Kibana
curl http://localhost:5601/api/status

# Test Logstash
echo '{"test": "message"}' | nc localhost 5000
```

### 3. Restart services

```bash
# Restart ELK stack
docker-compose restart elasticsearch kibana logstash

# Restart táº¥t cáº£
docker-compose restart
```

## ğŸ“ˆ Performance Optimization

### 1. Elasticsearch Tuning

```yaml
# ThÃªm vÃ o docker-compose.yml
environment:
  - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
  - discovery.type=single-node
  - xpack.security.enabled=false
```

### 2. Logstash Tuning

```yaml
# ThÃªm vÃ o docker-compose.yml
environment:
  - "LS_JAVA_OPTS=-Xmx512m -Xms512m"
```

### 3. Index Management

```bash
# Táº¡o index template
curl -X PUT "localhost:9200/_index_template/api-logs" -H 'Content-Type: application/json' -d'
{
  "index_patterns": ["api-logs-*"],
  "template": {
    "settings": {
      "number_of_shards": 1,
      "number_of_replicas": 0
    }
  }
}'
```

## ğŸ”„ Log Rotation

### 1. Elasticsearch Index Lifecycle

```bash
# Táº¡o lifecycle policy
curl -X PUT "localhost:9200/_ilm/policy/api-logs-policy" -H 'Content-Type: application/json' -d'
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_size": "1GB",
            "max_age": "7d"
          }
        }
      },
      "delete": {
        "min_age": "30d"
      }
    }
  }
}'
```

### 2. Logstash Buffer

```ruby
# ThÃªm vÃ o logstash.conf
input {
  tcp {
    port => 5000
    codec => json_lines
    buffer_size => 8192
  }
}
```

## ğŸ¯ Káº¿t luáº­n

Vá»›i setup nÃ y, báº¡n cÃ³:

âœ… **Real-time logging** cho 4 API services  
âœ… **Centralized log management** vá»›i Elasticsearch  
âœ… **Beautiful dashboards** vá»›i Kibana  
âœ… **Advanced log analysis** vÃ  search  
âœ… **Performance monitoring** qua logs  
âœ… **Error tracking** vÃ  debugging  

Dashboard sáº½ hiá»ƒn thá»‹ logs thá»±c tá»« cÃ¡c API services, giÃºp báº¡n monitor hiá»‡u suáº¥t vÃ  phÃ¡t hiá»‡n váº¥n Ä‘á» ká»‹p thá»i.

---

**ğŸ‰ ChÃºc má»«ng! Báº¡n Ä‘Ã£ setup thÃ nh cÃ´ng ELK Stack logging system!**
